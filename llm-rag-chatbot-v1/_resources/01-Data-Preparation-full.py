# Databricks notebook source
# MAGIC %md 
# MAGIC #1 Data collection and preparation
# MAGIC In this notebook, we'll download Databricks Documentation and use this as our Dataset.
# MAGIC
# MAGIC To generate our Training & Testing Dataset containing question and answer for each documentation section, we'll be using Databricks `AI_GENERATE_TEXT` and ask an external model to ask a question and answer it (make sure you check the LLM license used by your SQL AI function).
# MAGIC
# MAGIC *Note: For model fine-tuning, this is typically something you'd instead do manually, using real/human questions and answers. We will instead generate them to simplify our demo.*

# COMMAND ----------

# MAGIC %pip install beautifulsoup4 tiktoken

# COMMAND ----------

# MAGIC %run ./_resources/00-init $catalog=dbdemos $db=chatbot $reset_all_data=false

# COMMAND ----------

# MAGIC %md 
# MAGIC ## Extracting databricks documentation sitemap & pages
# MAGIC
# MAGIC Let's parse docs.databricks.com website and download the html content.

# COMMAND ----------

import requests
import xml.etree.ElementTree as ET

# Fetch the XML content from sitemap
response = requests.get("https://docs.databricks.com/en/doc-sitemap.xml")
root = ET.fromstring(response.content)
# Find all 'loc' elements (URLs) in the XML
urls = [loc.text for loc in root.findall(".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc")]
print(f"{len(urls)} Databricks documentation pages found")

#Let's split to 100 documentation page to make this demo faster:
urls = urls[:100]

# COMMAND ----------

# DBTITLE 1,Download Databricks Documentation HTML pages
import requests
import pandas as pd
import concurrent.futures
from bs4 import BeautifulSoup
import re

# Function to fetch HTML content for a given URL
def fetch_html(url):
    try:
        response = requests.get(url)
        response.raise_for_status()
        return response.content
    except Exception as e:
        print(f"Error fetching {url}: {e}")
        return None

# Function to process a URL and extract text from the specified div
def process_url(url):
    html_content = fetch_html(url)
    if html_content:
        soup = BeautifulSoup(html_content, "html.parser")
        article_div = soup.find("div", itemprop="articleBody")
        if article_div:
            article_text = str(article_div)
            return {"url": url, "text": article_text.strip()}
    return None

# Use a ThreadPoolExecutor with 10 workers
with concurrent.futures.ThreadPoolExecutor(max_workers=50) as executor:
    results = list(executor.map(process_url, urls))

# Filter out None values (URLs that couldn't be fetched or didn't have the specified div)
valid_results = [result for result in results if result is not None]

#Save the content in a raw table
spark.createDataFrame(valid_results).write.saveAsTable('raw_documentation')
display(spark.table('raw_documentation').limit(2))

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE IF NOT EXISTS databricks_documentation  (id BIGINT GENERATED BY DEFAULT AS IDENTITY, url STRING, content STRING, title STRING);

# COMMAND ----------

# MAGIC %md 
# MAGIC ### Splitting documentation pages in small chunks
# MAGIC LLM models typically have a maximum window size and you won't be able to compute embbeddings for too long texts.
# MAGIC
# MAGIC In addition, the bigger your context is, the longer your inference will be.
# MAGIC
# MAGIC Data preparation is key for your model to perform well and multiple strategy exist depending of your dataset:
# MAGIC
# MAGIC - Split document in small chunks (paragraph, h2...)
# MAGIC - Truncate documents to a fix number
# MAGIC - It could sometime make sense to split in big chunks ans ask a model to summurize each chunks as a one-off job for faster live inference.
# MAGIC
# MAGIC The number of token depends of your model. LLMs are shipped with a Tokenizer that you can use to count how many tokens will be created for a given sequence (usually > number of words) (see [hugging face documentation](https://huggingface.co/docs/transformers/main/tokenizer_summary) or [open AI](https://github.com/openai/tiktoken))
# MAGIC
# MAGIC
# MAGIC Make sure the tokenizer and context size limit you'll be using here matches your embedding model. Let's try an exemple with GPT3.5 tokenizer: `tiktoken.encoding_for_model("gpt-3.5-turbo")`

# COMMAND ----------

# DBTITLE 1,Counting our tokens using tiktoken
import tiktoken
#Create our tokenizer
tokenizer = tiktoken.encoding_for_model("gpt-3.5-turbo")

#Truncate the given text to the number of token.
def truncate(text, tokenizer, max_tokens = 4000):
    token_count = len(tokenizer.encode(text))y
    if token_count <= max_tokens:
        return text
        # Tokenize the text to get the tokens
    tokens = tokenizer.encode(text)
    # Truncate the tokens to the desired max_tokens
    truncated_tokens = tokens[:max_tokens]
    # Convert tokens back to text
    return tokenizer.decode(truncated_tokens)
  
sentence = "Hello, How are you? We are building an api for a chatbot."

token_count = len(tokenizer.encode(sentence))
print(f"This sentence has {token_count} tokens")
truncated_sentence = truncate(sentence, tokenizer, max_tokens = 10)
print(f"truncated to 10 tokens: {truncated_sentence}")  

# COMMAND ----------

# MAGIC %md 
# MAGIC ### Splitting our big documentation page in smaller chunks (h2 sections)
# MAGIC
# MAGIC In this demo, we have a few big documentation article, too big for our models. We'll split these articles between HTML h2 chunks, and ensure that each chunk isn't bigger than 4000 tokens.<br/>
# MAGIC To do so, we'll be using the `tiktoken` librairy to count gtp3.5 tokens. 
# MAGIC
# MAGIC Let's also remove the HTML tags to send plain text to our model.

# COMMAND ----------

from bs4 import BeautifulSoup

# Remove multiple line breaks and truncate the model
def cleanup_and_truncate_text(text, tokenizer, max_tokens = 4000):
    return truncate(re.sub(r'\n{3,}', '\n\n', text).strip(), tokenizer, max_tokens)
    
#Split the text in chunk between 1000 and 4000 tokens
#This consider that our sections between H2 are of decent size (not > 4000 tokens), which is the case with our corpus. 
#H2 Sections longer than 4000 will be truncated.
def split_html_by_h2(soup, html_content, tokenizer, max_tokens = 4000, min_chunk_size = 1000):
    chunks = []
    last_index = 0
    for element in soup.find_all(['h2']):
        h2_position = html_content.find(str(element))
        chunk = html_content[last_index:h2_position]
        chunk_text = BeautifulSoup(chunk).get_text()
        # Split on the next H2 only if we have more than half the max. 
        # This prevents from having too small chunks
        if len(tokenizer.encode(chunk_text)) > max_tokens/2:
            last_index = h2_position
            chunks.append(cleanup_and_truncate_text(chunk_text, tokenizer, max_tokens))
    #Append the last chunk
    chunk = html_content[last_index:]
    chunk_text = BeautifulSoup(chunk).get_text()
    if len(tokenizer.encode(chunk_text)) > min_chunk_size:
        chunks.append(cleanup_and_truncate_text(chunk_text, tokenizer, max_tokens))
    return chunks
  
#Let's try to split our doc between h2:
doc = """<h1>This is a title</h1>
<h2>Subtitle 1</h2>
Some description 1
<h2>Subtitle 2</h2>
And description 2"""

for split in split_html_by_h2(BeautifulSoup(doc), doc, tokenizer, 20, 3):
  print(split)
  print("---------")

# COMMAND ----------

# MAGIC %md 
# MAGIC Let's now split our entire dataset using this functio and a pandas UDF.
# MAGIC
# MAGIC We will also extract the title from the page (based on h1)

# COMMAND ----------


#Count number of token, if we exceed our limit will try to split it based on h2, or truncate it if no h2 exists.
def split_text(text, tokenizer, max_tokens = 4000, min_chunk_size = 100):
    soup = BeautifulSoup(text)
    txt = soup.get_text()
    token_count = len(tokenizer.encode(txt))
    if token_count > max_tokens:
        return split_html_by_h2(soup, text, tokenizer, max_tokens, min_chunk_size)
    else:
        return [re.sub(r'\n{3,}', '\n\n', txt).strip()]

#transform the html as text chunks. Will cut to 4000 tokens
@pandas_udf("array<string>")
def extract_chunks(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:
    # Load the model tokenizer
    tokenizer = tiktoken.encoding_for_model("gpt-3.5-turbo")
    for serie in iterator:
        # get a summary for each row
        yield serie.apply(lambda x: split_text(x, tokenizer, max_tokens = 4000, min_chunk_size = 100))
        

@pandas_udf("string")
def clean_html(serie):
    return serie.apply(lambda x: BeautifulSoup(x).get_text())

df = (spark.read.parquet("/dbdemos/chatbot-llm/documentation.parquet")
      # Define the regular expression pattern
        .withColumn("title", F.regexp_extract(col("text"), "<h1>(.*?)<\/h1>", 1))
        .withColumn("title", clean_html(col("title")))
        .withColumn('content', extract_chunks(col('text')))
        .drop('text'))
df = df.withColumn('content', F.explode('content'))
#display(df)
df.write.mode('overwrite').saveAsTable('databricks_documentation')

# COMMAND ----------

display(spark.table("databricks_documentation"))

# COMMAND ----------

# MAGIC %md
# MAGIC ## Let's use Databricks AI_GENERATE_TEXT to generate Questions for each documentation page  

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE IF NOT EXISTS training_dataset_question (id BIGINT GENERATED ALWAYS AS IDENTITY, doc_id BIGINT, question STRING);
# MAGIC CREATE TABLE IF NOT EXISTS training_dataset_answer   (id BIGINT GENERATED ALWAYS AS IDENTITY, question_id BIGINT, answer STRING);

# COMMAND ----------

# DBTITLE 1,Helper function to 
#See companion notebook
sql_api = SQLStatementAPI(warehouse_name = "test-llm", catalog = catalog, schema = dbName)

#Create an ASK_OPEN_AI wrapper function
sql_api.execute_sql("""CREATE OR REPLACE FUNCTION ASK_OPEN_AI(prompt STRING)
                        RETURNS STRING
                        RETURN 
                          AI_GENERATE_TEXT(prompt,
                                           "azure_openai/gpt-35-turbo",
                                           "apiKey", SECRET("dbdemos", "azure-openai"),
                                           "temperature", CAST(0.0 AS DOUBLE),
                                           "deploymentName", "dbdemo-gpt35",
                                           "resourceName", "dbdemos-open-ai",
                                           "apiVersion", "2023-03-15-preview")""")

# COMMAND ----------

sql_api.execute_sql("""
  CREATE OR REPLACE FUNCTION GENERATE_QUESTIONS_FROM_DOC(doc STRING)
  RETURNS array<STRING>
  RETURN from_json(ASK_OPEN_AI(
    CONCAT("Here is a documentation page from Databricks: ", doc, "Write one or two questions a developer, data sciencist or a devops might ask themselve, which can be answered from the above  documentation. Generate randomly one or two questions as a json array, no additional text."))
  , 'array<string>')""")

# COMMAND ----------

#TODO: temp, should be improved, temporary split for testing only to continue on potential error (context size etc), as calling openai is slow & expensive 
#spark.sql("delete from training_dataset_question")
for i in range(0, spark.table("databricks_documentation").count(), 10):
    print(f"processing rows {i}...")
    spark.sql(f'select * from databricks_documentation d limit 10 offset {i}').write.mode('overwrite').saveAsTable('databricks_documentation_tmp')
    sql_api.execute_sql("""
        INSERT INTO training_dataset_question (doc_id, question) (
              select (doc_id, explode(question) as question from (
                  select d.id as doc_id, GENERATE_QUESTIONS_FROM_DOC(d.content) as question from databricks_documentation_tmp d))""")
spark.sql("drop table databricks_documentation_tmp")

# COMMAND ----------

# MAGIC %sql select * from training_dataset_question

# COMMAND ----------

# MAGIC %md
# MAGIC ## Generate the Answers based on the doc & the questions

# COMMAND ----------

sql_api.execute_sql("""
  CREATE OR REPLACE FUNCTION GENERATE_ANSWER(doc STRING, question STRING)
  RETURNS STRING
  RETURN ASK_OPEN_AI(
    CONCAT("You are an assistant for python, spark, data engineering and data science on Databricks. Here is a documentation page with potential relevant informations: \n\n", doc, "\n\nAnswer the following question: \n\n", question))
  """)

# COMMAND ----------

# DBTITLE 1,Generate answers with doc + question
#temp debugging, should be improved, temporary split for testing only to continue on potential error (context size etc), as calling openai is slow & expensive 
#spark.sql("delete from training_dataset_question")
for i in range(1520, spark.table("training_dataset_question").count(), 10):
    print(f"processing rows {i}...")
    spark.sql(f'select q.question question, q.id as question_id, d.content from training_dataset_question q inner join databricks_documentation d on q.doc_id = d.id order by question_id asc limit 10 offset {i}').write.mode('overwrite').saveAsTable('databricks_question_chunk_tmp')
    sql_api.execute_sql("""
        INSERT INTO training_dataset_answer (question_id, answer) (
              select d.question_id, GENERATE_ANSWER(d.content, d.question) as answer from databricks_question_chunk_tmp d)""")
   
spark.sql("drop table databricks_question_chunk_tmp")

# COMMAND ----------

# MAGIC %sql select * from training_dataset_answer
